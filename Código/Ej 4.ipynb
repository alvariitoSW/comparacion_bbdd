{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ej4gen import get_users, get_cars\n",
    "import pandas as pd\n",
    "from pymemcache.client import base\n",
    "from pymemcache import serde\n",
    "from abc import ABC, abstractmethod\n",
    "import pymongo\n",
    "import sqlite3\n",
    "import psycopg2\n",
    "import duckdb\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from functools import partial\n",
    "import re\n",
    "import warnings\n",
    "from memory_profiler import memory_usage\n",
    "import math\n",
    "from copy import copy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cache = base.Client(('127.0.0.1', 11211),\n",
    "    serde=serde.pickle_serde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase abstracta con la funcionalidad común de las bases de datos\n",
    "class DB(ABC):\n",
    "    def __init__(self, dbname, table_name, unique_cols, query_type=\"sql\", close_conn_after_op=False):\n",
    "        self.dbname = dbname\n",
    "        self.table_name = table_name\n",
    "        self._unique_cols = unique_cols or []\n",
    "        self.oid = 0\n",
    "        self._cached_queries = set()\n",
    "        self._query_type = query_type\n",
    "        self.close_conn_after_op = close_conn_after_op\n",
    "        if self.close_conn_after_op:\n",
    "            try:\n",
    "                self._open_connection()\n",
    "                self._delete_table()\n",
    "            finally:\n",
    "                self._close_connection()\n",
    "        else:\n",
    "            self._open_connection()\n",
    "            self._delete_table()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _delete_table(self):\n",
    "        pass\n",
    "\n",
    "    def _get_cache_key(self, query):\n",
    "        # La llave de la caché usa el nombre de la implementación concreta para poder hacer pruebas con todas a la vez\n",
    "        return self.__class__.__name__ + \"/\" + re.sub(r'\\s+', '#', str(query))\n",
    "\n",
    "    def _clear_cache(self):\n",
    "        for key in list(self._cached_queries):\n",
    "            cache.delete(key)\n",
    "        self._cached_queries = set()\n",
    "\n",
    "    # Insert de la interfaz\n",
    "    def insert(self, data_csv, data_json, *_, oid=None, batch_size=None, **__):\n",
    "        if oid is None or oid >= self.oid:\n",
    "            self.oid += 1\n",
    "            self._clear_cache()\n",
    "            if batch_size is None:\n",
    "                batch_size = len(data_json) # Si no se especifica un tamaño de batch, insertar todo a la vez\n",
    "            return self._insert(data_csv, data_json, batch_size)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _insert(self, data_csv, data_json, batch_size):\n",
    "        pass\n",
    "\n",
    "    def read(self, *_, oid=None, query=None, use_cache=True, **__):\n",
    "        if oid is None or oid >= self.oid:\n",
    "            self.oid += 1\n",
    "            # Usar un select all como query predeterminada\n",
    "            query = query or (f'SELECT * FROM {self.table_name};' if self._query_type == \"sql\" else {})\n",
    "            key = self._get_cache_key(query)\n",
    "            cached_data = cache.get(key)\n",
    "            if use_cache and cached_data is not None:\n",
    "                return cached_data\n",
    "            if self.close_conn_after_op:\n",
    "                try:\n",
    "                    self._open_connection()\n",
    "                    data = self._read(query)\n",
    "                finally:\n",
    "                    self._close_connection()\n",
    "            else:\n",
    "                data = self._read(query)\n",
    "            cache.set(key, data)\n",
    "            self._cached_queries.add(key)\n",
    "            return data\n",
    "\n",
    "    @abstractmethod\n",
    "    def _read(self, query):\n",
    "        pass\n",
    "\n",
    "    def update(self, *_, oid=None, **__):\n",
    "        if oid is None or oid >= self.oid:\n",
    "            self.oid += 1\n",
    "            self._clear_cache()\n",
    "            self._update()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _update(self):\n",
    "        pass\n",
    "\n",
    "    def to_df(self):\n",
    "        if self.close_conn_after_op:\n",
    "            try:\n",
    "                self._open_connection()\n",
    "                return self._to_df()\n",
    "            finally:\n",
    "                self._close_connection()\n",
    "        else:\n",
    "            return self._to_df()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _to_df(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _open_connection(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _close_connection(self):\n",
    "        pass\n",
    "\n",
    "    def create_index(self, index_cols=None):\n",
    "        if index_cols is None:\n",
    "            index_cols = self._unique_cols\n",
    "\n",
    "        if self.close_conn_after_op:\n",
    "            try:\n",
    "                self._open_connection()\n",
    "                return self._create_index(index_cols)\n",
    "            finally:\n",
    "                self._close_connection()\n",
    "        else:\n",
    "            return self._create_index(index_cols)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_index(self, index_cols):\n",
    "        pass\n",
    "\n",
    "    def __del__(self):\n",
    "        self._clear_cache()\n",
    "        if not self.close_conn_after_op:\n",
    "            self._close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para calcular tiempos en el gestor de MongoDB\n",
    "class MongoDB(DB):\n",
    "    def __init__(self, dbname, table_name, unique_cols=None, close_conn_after_op=False):\n",
    "        super().__init__(dbname, table_name, unique_cols, query_type=\"mongo\", close_conn_after_op=close_conn_after_op)\n",
    "\n",
    "    def _delete_table(self):\n",
    "        self.db.drop_collection(self.table_name)\n",
    "\n",
    "    def _insert(self, _, data_json, batch_size):\n",
    "        if self.close_conn_after_op:\n",
    "            for i in range(0, len(data_json), batch_size):\n",
    "                try:\n",
    "                    self._open_connection()\n",
    "                    self.collection.insert_many(data_json[i:i+batch_size]) # Inserta los registros en batches\n",
    "                finally:\n",
    "                    self._close_connection()\n",
    "        else:\n",
    "            for i in range(0, len(data_json), batch_size):\n",
    "                self.collection.insert_many(data_json[i:i+batch_size]) # Inserta los registros en batches\n",
    "\n",
    "    def _read(self, query):\n",
    "        if isinstance(query, list): # Detecta si debe hacer un aggregate, como es el caso del join\n",
    "            return list(self.collection.aggregate(query))\n",
    "        return list(self.collection.find(query))\n",
    "\n",
    "    def _update(self):\n",
    "        \"\"\"\n",
    "        Actualiza el campo 'dni' añadiendo un 0 al final de cada valor para todos los documentos en la colección.\n",
    "        La actualización se realiza de una sola vez para todos los documentos.\n",
    "        \"\"\"\n",
    "        # Busca todos los documentos y selecciona el campo 'dni'\n",
    "        if self.close_conn_after_op:\n",
    "            self._open_connection()\n",
    "        documents = self.collection.find({}, {\"dni\": 1})\n",
    "\n",
    "        bulk_updates = []\n",
    "\n",
    "        for document in documents:\n",
    "            current_dni = document.get('dni', '')\n",
    "            new_dni = str(current_dni) + '0'\n",
    "\n",
    "            # Preparar la operación de actualización en bloque\n",
    "            bulk_updates.append(\n",
    "                pymongo.UpdateOne(\n",
    "                    {\"_id\": document[\"_id\"]}, # Filtra por el ID del documento (siempre indexado)\n",
    "                    {\"$set\": {\"dni\": new_dni}} # Establece el nuevo valor de 'dni'\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        if self.close_conn_after_op:\n",
    "            self._close_connection()\n",
    "\n",
    "        # Ejecuta las operaciones de una en una para medir la diferencia en rendimiento respecto a la conexión\n",
    "        if bulk_updates:\n",
    "            if self.close_conn_after_op:\n",
    "                for op in bulk_updates:\n",
    "                    try:\n",
    "                        self._open_connection()\n",
    "                        self.collection.bulk_write([op])\n",
    "                    finally:\n",
    "                        self._close_connection()\n",
    "            else:\n",
    "                for op in bulk_updates:\n",
    "                    self.collection.bulk_write([op])\n",
    "\n",
    "    def _close_connection(self):\n",
    "        if self._client:\n",
    "            self._client.close()\n",
    "            self._client = None\n",
    "\n",
    "    def _open_connection(self):\n",
    "        self._client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "        self.db = self._client[self.dbname]\n",
    "        self.collection = self.db[self.table_name]\n",
    "\n",
    "    def _to_df(self):\n",
    "        return pd.DataFrame(self.read()) # Usa la query predeterminada para obtener la base de datos entera\n",
    "\n",
    "    def _create_index(self, index_cols):\n",
    "        for col in index_cols:\n",
    "            self.collection.create_index([(col, pymongo.ASCENDING)], unique=True)\n",
    "\n",
    "\n",
    "# Clase para calcular tiempos en el gestor de PostgreSQL\n",
    "class PostgresqlDB(DB):\n",
    "    def __init__(self, dbname, table_name, unique_cols=None, close_conn_after_op=False):\n",
    "        super().__init__(dbname, table_name, unique_cols, close_conn_after_op=close_conn_after_op)\n",
    "\n",
    "    def _delete_table(self):\n",
    "        self.cursor.execute(text(f\"DROP TABLE IF EXISTS {self.table_name};\")) # Limpiar la tabla antes de insertar\n",
    "        self.cursor.commit()\n",
    "\n",
    "    def _create_table(self, data_csv):\n",
    "        \"\"\"\n",
    "        Create a table based on the DataFrame's columns and types, with unique constraints on specified columns.\n",
    "        \"\"\"\n",
    "        # Mapping de pandas a PostgreSQL\n",
    "        type_mapping = {\n",
    "            'int64': 'INTEGER',\n",
    "            'float64': 'FLOAT',\n",
    "            'object': 'TEXT',\n",
    "            'bool': 'BOOLEAN',\n",
    "            'datetime64[ns]': 'TIMESTAMP',\n",
    "            'timedelta[ns]': 'INTERVAL'\n",
    "        }\n",
    "\n",
    "        columns_with_types = []\n",
    "        for col, dtype in data_csv.dtypes.items():\n",
    "            sql_type = type_mapping.get(str(dtype), 'TEXT') # Usar TEXT por defecto\n",
    "            columns_with_types.append(f\"{col} {sql_type}\")\n",
    "\n",
    "        # if self._unique_cols:\n",
    "        #     unique_str = \", \".join([f\"UNIQUE({col})\" for col in self._unique_cols])\n",
    "        #     columns_with_types.append(unique_str)\n",
    "\n",
    "        # Generar el texto de creación de la tabla\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {self.table_name} (\n",
    "            {', '.join(columns_with_types)}\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        self.cursor.execute(text(create_table_query))\n",
    "        self.cursor.commit()\n",
    "\n",
    "    def _insert(self, data_csv, _, batch_size):\n",
    "        \"\"\"\n",
    "        Inserta los datos de un DataFrame de pandas (ya cargado con read_csv) en la tabla de PostgreSQL.\n",
    "        \"\"\"\n",
    "        if self.close_conn_after_op:\n",
    "            try:\n",
    "                self._open_connection()\n",
    "                self._create_table(data_csv)\n",
    "            finally:\n",
    "                self._close_connection()\n",
    "        else:\n",
    "            self._create_table(data_csv)\n",
    "        \n",
    "        # Query para insertar valores\n",
    "        columns = ', '.join(data_csv.columns)\n",
    "        placeholders = ', '.join([f':{col}' for col in data_csv.columns])\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {self.table_name} ({columns})\n",
    "        VALUES ({placeholders})\n",
    "        \"\"\"\n",
    "        \n",
    "        data = data_csv.to_dict('records')\n",
    "        \n",
    "        if self.close_conn_after_op: # Insertar por batches\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                try:\n",
    "                    self._open_connection()\n",
    "                    self.cursor.execute(text(insert_query), data[i:i+batch_size])\n",
    "                    self.cursor.commit()\n",
    "                finally:\n",
    "                    self._close_connection()\n",
    "        else:\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                self.cursor.execute(text(insert_query), data[i:i+batch_size])\n",
    "                self.cursor.commit()\n",
    "\n",
    "    def _read(self, query):\n",
    "        return pd.read_sql_query(query, self.conn)\n",
    "\n",
    "    def _update(self):\n",
    "        \"\"\"\n",
    "        Actualiza el campo 'dni' añadiendo un 0 al final de cada valor para cada registro de manera individual.\n",
    "        \"\"\"\n",
    "        select_query = text(f\"SELECT dni FROM {self.table_name};\")\n",
    "\n",
    "        # Se obtienen todos los DNIs\n",
    "        if self.close_conn_after_op:\n",
    "            try:\n",
    "                self._open_connection()\n",
    "                result = self.cursor.execute(select_query)\n",
    "                dni_list = result.fetchall()\n",
    "            finally:\n",
    "                self._close_connection()\n",
    "        else:\n",
    "            result = self.cursor.execute(select_query)\n",
    "            dni_list = result.fetchall()\n",
    "\n",
    "        # Se actualiza cada DNI individualmente para poder comparar el rendimiento según la conexión\n",
    "        for dni in dni_list:\n",
    "            updated_dni = dni[0] + '0'\n",
    "\n",
    "            if self.close_conn_after_op:\n",
    "                try:\n",
    "                    self._open_connection()\n",
    "                    update_query = text(f\"UPDATE {self.table_name} SET dni = :updated_dni WHERE dni = :original_dni;\")\n",
    "                    self.cursor.execute(update_query, {'updated_dni': updated_dni, 'original_dni': dni[0]})\n",
    "                    self.cursor.commit()\n",
    "                finally:\n",
    "                    self._close_connection()\n",
    "            else:\n",
    "                update_query = text(f\"UPDATE {self.table_name} SET dni = :updated_dni WHERE dni = :original_dni;\")\n",
    "                self.cursor.execute(update_query, {'updated_dni': updated_dni, 'original_dni': dni[0]})\n",
    "                self.cursor.commit()\n",
    "\n",
    "    def _close_connection(self):\n",
    "        if self.cursor:\n",
    "            self.cursor.close()\n",
    "            self.cursor = None\n",
    "\n",
    "    def _open_connection(self):\n",
    "        self.conn = create_engine(f'postgresql+psycopg2://postgres:postgres@localhost:5432/{self.dbname}')\n",
    "        self.cursor = self.conn.connect()\n",
    "\n",
    "    def _to_df(self):\n",
    "        return pd.read_sql_query(f'SELECT * FROM {self.table_name};', self.conn)\n",
    "\n",
    "    def _create_index(self, index_cols=None):\n",
    "        for col in index_cols:\n",
    "            index_query = f\"CREATE INDEX idx_{self.table_name}_{col} ON {self.table_name} ({col});\"\n",
    "            self.cursor.execute(text(index_query))\n",
    "        self.cursor.commit()\n",
    "\n",
    "\n",
    "# Clase para calcular tiempos en el gestor de Sqlite3DB\n",
    "class Sqlite3DB(DB):\n",
    "    def __init__(self, dbname, table_name, unique_cols=None, close_conn_after_op=False):\n",
    "        super().__init__(dbname + \".sqlite3\", table_name, unique_cols, close_conn_after_op=close_conn_after_op)\n",
    "\n",
    "    def _delete_table(self):\n",
    "        self.conn.execute(f\"DROP TABLE IF EXISTS {self.table_name};\") # Limpiar la tabla antes de insertar\n",
    "        self.conn.commit()  # Aplicar los cambios\n",
    "\n",
    "    def _create_table(self, data_csv):\n",
    "        \"\"\"\n",
    "        Create a table based on the DataFrame's columns and types, with unique constraints on specified columns.\n",
    "        \"\"\"\n",
    "        # Mapping de pandas a SQLite\n",
    "        type_mapping = {\n",
    "            'int64': 'INTEGER',\n",
    "            'float64': 'REAL',\n",
    "            'object': 'TEXT',\n",
    "            'bool': 'BOOLEAN',\n",
    "            'datetime64[ns]': 'TEXT'\n",
    "        }\n",
    "\n",
    "        columns_with_types = []\n",
    "        for col, dtype in data_csv.dtypes.items():\n",
    "            sql_type = type_mapping.get(str(dtype), 'TEXT') # Usar TEXT por defecto\n",
    "            columns_with_types.append(f\"{col} {sql_type}\")\n",
    "\n",
    "        # if self._unique_cols:\n",
    "        #     unique_str = \", \".join([f\"UNIQUE({col})\" for col in self._unique_cols])\n",
    "        #     columns_with_types.append(unique_str)\n",
    "\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {self.table_name} (\n",
    "            {', '.join(columns_with_types)}\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        self.conn.execute(create_table_query)\n",
    "        self.conn.commit()\n",
    "\n",
    "    def _insert(self, data_csv, _, batch_size):\n",
    "        if self.close_conn_after_op:\n",
    "            try:\n",
    "                self._open_connection()\n",
    "                self._create_table(data_csv)\n",
    "            finally:\n",
    "                self._close_connection()\n",
    "        else:\n",
    "            self._create_table(data_csv)\n",
    "        \n",
    "        columns = ', '.join(data_csv.columns)\n",
    "        placeholders = ', '.join(['?' for _ in data_csv.columns])\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {self.table_name} ({columns})\n",
    "        VALUES ({placeholders})\n",
    "        \"\"\"\n",
    "        \n",
    "        data = [tuple(x) for x in data_csv.to_numpy()]\n",
    "        \n",
    "        if self.close_conn_after_op: # Insertar por batches\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                try:\n",
    "                    self._open_connection()\n",
    "                    self.cursor.executemany(insert_query, data[i:i+batch_size])\n",
    "                    self.conn.commit()\n",
    "                finally:\n",
    "                    self._close_connection()\n",
    "        else:\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                self.cursor.executemany(insert_query, data[i:i+batch_size])\n",
    "                self.conn.commit()\n",
    "\n",
    "    def _read(self, query):\n",
    "        self.cursor.execute(query)\n",
    "        return self.cursor.fetchall()\n",
    "\n",
    "    def _update(self):\n",
    "        \"\"\"\n",
    "        Actualiza el campo 'dni' añadiendo un 0 al final de cada valor para cada registro de manera individual.\n",
    "        \"\"\"\n",
    "        select_query = f\"SELECT dni FROM {self.table_name};\"\n",
    "        \n",
    "        # Se obtienen todos los DNIs\n",
    "        if self.close_conn_after_op:\n",
    "            try:\n",
    "                self._open_connection()\n",
    "                self.cursor.execute(select_query)\n",
    "                dni_list = self.cursor.fetchall()\n",
    "            finally:\n",
    "                self._close_connection()\n",
    "        else:\n",
    "            self.cursor.execute(select_query)\n",
    "            dni_list = self.cursor.fetchall()\n",
    "\n",
    "        # Se actualiza cada DNI individualmente para poder comparar el rendimiento según la conexión\n",
    "        for dni in dni_list:\n",
    "            updated_dni = dni[0] + '0'\n",
    "\n",
    "            if self.close_conn_after_op:\n",
    "                try:\n",
    "                    self._open_connection()\n",
    "                    update_query = f\"UPDATE {self.table_name} SET dni = :updated_dni WHERE dni = :original_dni;\"\n",
    "                    self.cursor.execute(update_query, {'updated_dni': updated_dni, 'original_dni': dni[0]})\n",
    "                    self.conn.commit()\n",
    "                finally:\n",
    "                    self._close_connection()\n",
    "            else:\n",
    "                update_query = f\"UPDATE {self.table_name} SET dni = :updated_dni WHERE dni = :original_dni;\"\n",
    "                self.cursor.execute(update_query, {'updated_dni': updated_dni, 'original_dni': dni[0]})\n",
    "                self.conn.commit()\n",
    "\n",
    "    def _close_connection(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            self.conn = None\n",
    "\n",
    "    def _open_connection(self):\n",
    "        self.conn = sqlite3.connect(self.dbname)\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "    def _to_df(self):\n",
    "        # Convertir los resultados en un DataFrame\n",
    "        return pd.DataFrame(self.read(), columns=[desc[0] for desc in self.cursor.description])\n",
    "\n",
    "    def _create_index(self, index_cols=None):\n",
    "        for col in index_cols:\n",
    "            index_query = f\"CREATE INDEX idx_{self.table_name}_{col} ON {self.table_name} ({col});\"\n",
    "            self.conn.execute(index_query)\n",
    "        self.conn.commit()\n",
    "\n",
    "\n",
    "# Clase para calcular tiempos en el gestor de DuckDB\n",
    "class DuckDB(DB):\n",
    "    def __init__(self, dbname, table_name, unique_cols=None, close_conn_after_op=False):\n",
    "        super().__init__(dbname, table_name, unique_cols, close_conn_after_op=close_conn_after_op)\n",
    "\n",
    "    def _delete_table(self):\n",
    "        self.conn.execute(f\"DROP TABLE IF EXISTS {self.table_name};\")\n",
    "\n",
    "    def _create_table(self, data_csv, index_cols=None):\n",
    "        \"\"\"\n",
    "        Create a table based on the DataFrame's columns and types, with unique constraints on specified columns.\n",
    "        \"\"\"\n",
    "        # Mapping de pandas a DuckDB\n",
    "        type_mapping = {\n",
    "            'int64': 'INTEGER',\n",
    "            'float64': 'DOUBLE',\n",
    "            'object': 'TEXT',\n",
    "            'bool': 'BOOLEAN',\n",
    "            'datetime64[ns]': 'TIMESTAMP'\n",
    "        }\n",
    "\n",
    "        columns_with_types = []\n",
    "        for col, dtype in data_csv.dtypes.items():\n",
    "            sql_type = type_mapping.get(str(dtype), 'TEXT') # Usar TEXT por defecto\n",
    "            columns_with_types.append(f\"{col} {sql_type}\")\n",
    "\n",
    "        if index_cols:\n",
    "            unique_str = \", \".join([f\"UNIQUE({col})\" for col in index_cols])\n",
    "            columns_with_types.append(unique_str)\n",
    "\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {self.table_name} (\n",
    "            {', '.join(columns_with_types)}\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        self.conn.execute(create_table_query)\n",
    "\n",
    "    def _insert(self, data_csv, _, batch_size, index_cols=None):\n",
    "        if self.close_conn_after_op:\n",
    "            try:\n",
    "                self._open_connection()\n",
    "                self._create_table(data_csv, index_cols)\n",
    "            finally:\n",
    "                self._close_connection()\n",
    "        else:\n",
    "            self._create_table(data_csv, index_cols)\n",
    "        self.data = data_csv\n",
    "        \n",
    "        columns = ', '.join(data_csv.columns)\n",
    "        placeholders = ', '.join(['?' for _ in data_csv.columns])\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {self.table_name} ({columns})\n",
    "        VALUES ({placeholders})\n",
    "        \"\"\"\n",
    "        \n",
    "        data = [tuple(x) for x in data_csv.to_numpy()]\n",
    "        \n",
    "        if self.close_conn_after_op: # Insertar por batches\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                try:\n",
    "                    self._open_connection()\n",
    "                    self.conn.executemany(insert_query, data[i:i+batch_size])\n",
    "                    self.conn.commit()\n",
    "                finally:\n",
    "                    self._close_connection()\n",
    "        else:\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                self.conn.executemany(insert_query, data[i:i+batch_size])\n",
    "                self.conn.commit()\n",
    "\n",
    "    def _read(self, query):\n",
    "        self.cursor.execute(query)\n",
    "        return self.cursor.fetchall()\n",
    "\n",
    "    def _update(self):\n",
    "        \"\"\"\n",
    "        Actualiza el campo 'dni' añadiendo un 0 al final de cada valor para cada registro de manera individual.\n",
    "        \"\"\"\n",
    "        select_query = f\"SELECT dni FROM {self.table_name};\"\n",
    "        \n",
    "        # Se obtienen todos los DNIs\n",
    "        if self.close_conn_after_op:\n",
    "            try:\n",
    "                self._open_connection()\n",
    "                self.cursor.execute(select_query)\n",
    "                dni_list = self.cursor.fetchall()\n",
    "            finally:\n",
    "                self._close_connection()\n",
    "        else:\n",
    "            self.cursor.execute(select_query)\n",
    "            dni_list = self.cursor.fetchall()\n",
    "\n",
    "        # Se actualiza cada DNI individualmente para poder comparar el rendimiento según la conexión\n",
    "        for dni in dni_list:\n",
    "            updated_dni = dni[0] + '0'\n",
    "\n",
    "            if self.close_conn_after_op:\n",
    "                try:\n",
    "                    self._open_connection()\n",
    "                    update_query = f\"UPDATE {self.table_name} SET dni = ? WHERE dni = ?;\"\n",
    "                    self.cursor.execute(update_query, (updated_dni, dni[0]))\n",
    "                    self.conn.commit()\n",
    "                finally:\n",
    "                    self._close_connection()\n",
    "            else:\n",
    "                update_query = f\"UPDATE {self.table_name} SET dni = ? WHERE dni = ?;\"\n",
    "                self.cursor.execute(update_query, (updated_dni, dni[0]))\n",
    "                self.conn.commit()\n",
    "\n",
    "    def _close_connection(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            self.conn = None\n",
    "\n",
    "    def _open_connection(self):\n",
    "        self.conn = duckdb.connect(self.dbname)\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "    def _to_df(self):\n",
    "        return pd.DataFrame(self.read(), columns=[desc[0] for desc in self.cursor.description])\n",
    "\n",
    "    def _create_index(self, index_cols=None):\n",
    "        self._delete_table()\n",
    "        self._insert(self.data, None, self.data.shape[0], index_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Measurements:\n",
    "    def __init__(self, classes, dbname, table_name, unique_cols, sizes):\n",
    "        self.classes = classes\n",
    "        self.sizes = sizes\n",
    "        self.dbname = dbname\n",
    "        self.table_name = table_name\n",
    "        self._unique_cols = unique_cols\n",
    "\n",
    "    def _get_data(self, n, dnis=None):\n",
    "        return get_users(n) if self.table_name == 'users' else get_cars(([] if dnis is None else dnis), n)\n",
    "\n",
    "    def _measure_time(self, function):\n",
    "        t1 = time.perf_counter(), time.process_time()\n",
    "        function()\n",
    "        t2 = time.perf_counter(), time.process_time()\n",
    "        return t2[0] - t1[0], t2[1] - t1[1]\n",
    "\n",
    "    def _measure_time_and_memory(self, function):\n",
    "        # Se mide la memoria de la función de medición de tiempos\n",
    "        # De hacerlo al revés, todas las mediciones tardarían unos 6 segundos más por el tiempo de inicialización de `memory_usage`\n",
    "        \n",
    "        # Se usa una lista para poder sacar los resultados de la función interna\n",
    "        result = []\n",
    "        def _measure_time(function):\n",
    "            t1 = time.perf_counter(), time.process_time()\n",
    "            function()\n",
    "            t2 = time.perf_counter(), time.process_time()\n",
    "            result.append((t2[0] - t1[0], t2[1] - t1[1]))\n",
    "        memory = memory_usage(lambda: _measure_time(function))\n",
    "        return result[0][0], result[0][1], np.average(memory)\n",
    "\n",
    "    def _get_inserts(self):\n",
    "        n = self.sizes[0]\n",
    "        data_csv, data_json = self._get_data(n)\n",
    "        results = {cl: {\"batches\": {\"close\": [], \"keep alive\": []}, \"standard\": {\"result\": []}} for cl in self.classes}\n",
    "\n",
    "        # Datos de tiempos según el tamaño del batch\n",
    "        for cl, cl_class in self.classes.items():\n",
    "            for close_conn in [False, True]:\n",
    "                print(f\"\\n{cl} {'close     ' if close_conn else 'keep alive'} {' ' * (20 - len(cl) - len(str(n)))}{n}\", end=\"\")\n",
    "                i = 1\n",
    "                while i < 2 * len(data_json):\n",
    "                    cl_obj = cl_class(self.dbname, self.table_name, self._unique_cols, close_conn_after_op=close_conn)\n",
    "                    print(f\" | {i if i < len(data_json) else 'full'} \", end=\"\")\n",
    "                    t_real, t_cpu, memory = self._measure_time_and_memory(partial(cl_obj.insert, data_csv, data_json, oid=cl_obj.oid, batch_size=i))\n",
    "                    results[cl][\"batches\"]['close' if close_conn else 'keep alive'].append((t_real, t_cpu, memory))\n",
    "                    i *= 2\n",
    "                    t_str = f\"{t_real:.5f}\"\n",
    "                    print(f\"{t_str:>10}s\", end=\"\")\n",
    "            results[cl][\"standard\"][\"result\"].append((t_real, t_cpu, memory))\n",
    "\n",
    "        # Datos de tiempos según el tamaño del dataset\n",
    "        for size in self.sizes[1:]:\n",
    "            data_csv, data_json = self._get_data(size)\n",
    "            for cl, cl_class in self.classes.items():\n",
    "                cl_obj = cl_class(self.dbname, self.table_name, self._unique_cols, close_conn_after_op=False) # False ya que no hay diferencia dado que se añade todo en un solo update\n",
    "                print(f\"\\n{cl}            {' ' * (20 - len(cl) - len(str(size)))}{size} | full \", end=\"\")\n",
    "                t_real, t_cpu, memory = self._measure_time_and_memory(partial(cl_obj.insert, data_csv, data_json, oid=cl_obj.oid))\n",
    "                results[cl][\"standard\"][\"result\"].append((t_real, t_cpu, memory))\n",
    "                t_str = f\"{t_real:.5f}\"\n",
    "                print(f\"{t_str:>10}s\", end=\"\")\n",
    "        print()\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_inserts(self):\n",
    "        results = self._get_inserts()\n",
    "        print(\"By batches\")\n",
    "        self._plot_results({cl: results[cl][\"batches\"] for cl in results}, [\"time_real\", \"time_cpu\", \"memory\"], x_axis=[2**n for n in range(0, math.ceil(math.log2(self.sizes[0])))] + [self.sizes[0]], log_base=2)\n",
    "        print(\"By sizes\")\n",
    "        self._plot_results({cl: results[cl][\"standard\"] for cl in results}, [\"time_real\", \"time_cpu\", \"memory\"])\n",
    "\n",
    "    def _get_reads(self):\n",
    "        table_name = self.table_name\n",
    "        try:\n",
    "            results = {cl: {\"standard\": {}, \"mix\": {}} for cl in self.classes}\n",
    "\n",
    "            for n in self.sizes:\n",
    "                self.table_name = \"users\"\n",
    "                data_csv_users, data_json_users = self._get_data(n)\n",
    "                self.table_name = \"cars\"\n",
    "                data_csv_cars, data_json_cars = self._get_data(n, dnis=[el[\"dni\"] for el in data_json_users])\n",
    "                data_mix = {el[\"dni\"]: copy(el) for el in data_json_users}\n",
    "                for car in data_json_cars:\n",
    "                    data_mix[car[\"dni\"]].setdefault(\"cars\", []).append(car)\n",
    "                data_mix = list(data_mix.values())\n",
    "\n",
    "                # Tiempo de read por base de datos, configuración (cache, index) y query\n",
    "                for cl, cl_class in self.classes.items():\n",
    "                    print(f\"\\n{cl} {' ' * (20 - len(cl) - len(str(n)))}{n}\", end=\"\")\n",
    "                    for conn in [\"keep alive\", \"close\"]:\n",
    "                        cl_obj2 = cl_class(self.dbname, \"cars\", [\"vin\", \"plate\"], close_conn_after_op=False) # False por eficiencia, ya que no se realizan mediciones sobre esto\n",
    "                        cl_obj2.insert(data_csv_cars, data_json_cars, oid=cl_obj2.oid)\n",
    "                        cl_obj = cl_class(self.dbname, \"users\", [\"dni\"], close_conn_after_op=(conn == \"close\"))\n",
    "                        if cl == \"mongo\":\n",
    "                            cl_obj3 = cl_class(self.dbname, \"mix\", [], close_conn_after_op=(conn == \"close\"))\n",
    "                            cl_obj3.insert(None, data_mix, oid=cl_obj3.oid)\n",
    "                        column = cl_obj._unique_cols[0]\n",
    "                        cl_obj.insert(data_csv_users, data_json_users, oid=cl_obj.oid)\n",
    "                        for cache_type in [\"no index\", \"index\", \"cache\"]:\n",
    "                            if cache_type == \"index\":\n",
    "                                cl_obj.create_index()\n",
    "                                cl_obj2.create_index()\n",
    "                                cl_obj3.create_index()\n",
    "\n",
    "                            print(f\" | standard {conn} ({cache_type}) \", end=\"\")\n",
    "                            t = time.time()\n",
    "                            times = []\n",
    "                            for value in data_csv_cars[column][-100:]:\n",
    "                                query = f\"SELECT * FROM table WHERE {column} = '{value}'\" if cl_obj._query_type == \"sql\" else {column: value}\n",
    "                                t_real, t_cpu = self._measure_time(partial(cl_obj.read, query, oid=cl_obj.oid, use_cache=(cache_type == \"cache\")))\n",
    "                                times.append((t_real, t_cpu))\n",
    "                            t_str = f\"{time.time() - t:.5f}\"\n",
    "                            print(f\"{t_str:>10}s\", end=\"\")\n",
    "                            results[cl][\"standard\"].setdefault(f\"{conn} ({cache_type})\", []).append((np.mean([t[0] for t in times]), np.mean([t[1] for t in times])))\n",
    "\n",
    "                            print(f\" | mix {conn} ({cache_type}) \", end=\"\")\n",
    "                            t = time.time()\n",
    "                            times = []\n",
    "                            times2 = []\n",
    "                            for value in data_csv_cars[column][-100:]:\n",
    "                                query = f\"\"\"\n",
    "                                    SELECT users.*, cars.*\n",
    "                                    FROM users\n",
    "                                    JOIN cars ON users.dni = cars.dni\n",
    "                                    WHERE users.{column} = '{value}'\n",
    "                                    \"\"\" if cl_obj._query_type == \"sql\" else [\n",
    "                                    { \"$match\": { column: value } },\n",
    "                                    { \"$lookup\": {\n",
    "                                        \"from\": \"cars\",\n",
    "                                        \"localField\": \"dni\",\n",
    "                                        \"foreignField\": \"dni\",\n",
    "                                        \"as\": \"cars\"\n",
    "                                    }}\n",
    "                                ]\n",
    "                                t_real, t_cpu = self._measure_time(partial(cl_obj.read, query, oid=cl_obj.oid, use_cache=(cache_type == \"cache\")))\n",
    "                                times.append((t_real, t_cpu))\n",
    "                                if cl == \"mongo\":\n",
    "                                    query = {\"dni\": value}\n",
    "                                    t_real, t_cpu = self._measure_time(partial(cl_obj3.read, query, oid=cl_obj.oid, use_cache=(cache_type == \"cache\")))\n",
    "                                    times2.append((t_real, t_cpu))\n",
    "                            t_str = f\"{time.time() - t:.5f}\"\n",
    "                            print(f\"{t_str:>10}s\", end=\"\")\n",
    "                            results[cl][\"mix\"].setdefault(f\"{conn} ({cache_type}) | join\", []).append((np.mean([t[0] for t in times]), np.mean([t[1] for t in times])))\n",
    "                            if cl == \"mongo\":\n",
    "                                results[cl][\"mix\"].setdefault(f\"{conn} ({cache_type}) | db\", []).append((np.mean([t[0] for t in times2]), np.mean([t[1] for t in times2])))\n",
    "            print()\n",
    "\n",
    "            return results\n",
    "        finally:\n",
    "            self.table_name = table_name\n",
    "\n",
    "    def plot_reads(self):\n",
    "        results = self._get_reads()\n",
    "        print(\"Normal read\")\n",
    "        self._plot_results({cl: results[cl][\"standard\"] for cl in results}, [\"time_real\", \"time_cpu\"])\n",
    "        print(\"Join read\")\n",
    "        self._plot_results({cl: results[cl][\"mix\"] for cl in results}, [\"time_real\", \"time_cpu\"])\n",
    "\n",
    "    def _get_updates(self):\n",
    "        results = {cl: {} for cl in self.classes}\n",
    "\n",
    "        # Tiempo y memoria de update por tamaño del dataset\n",
    "        for n in self.sizes:\n",
    "            data_csv, data_json = self._get_data(n)\n",
    "\n",
    "            for cl, cl_class in self.classes.items():\n",
    "                print(f\"\\n{cl} {' ' * (20 - len(cl) - len(str(n)))}{n}\", end=\"\")\n",
    "                for conn in [\"keep alive\", \"close\"]:\n",
    "                    cl_obj = cl_class(self.dbname, self.table_name, self._unique_cols, close_conn_after_op=(conn == \"close\"))\n",
    "                    oid = cl_obj.oid\n",
    "                    cl_obj.insert(data_csv, data_json, oid=oid)\n",
    "                    for index_type in [\"no index\", \"index\"]:\n",
    "                        if index_type == \"index\":\n",
    "                            cl_obj.create_index()\n",
    "                        \n",
    "                        print(f\" | {conn} ({index_type}) \", end=\"\")\n",
    "                        oid = cl_obj.oid\n",
    "                        t_real, t_cpu, memory = self._measure_time_and_memory(\n",
    "                            partial(cl_obj.update, oid=oid)\n",
    "                        )\n",
    "                        results[cl].setdefault(f\"{conn} ({index_type})\", []).append((t_real, t_cpu, memory))\n",
    "                        t_str = f\"{t_real:.5f}\"\n",
    "                        print(f\"{t_str:>10}s\", end=\"\")\n",
    "        print()\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_updates(self):\n",
    "        results = self._get_updates()\n",
    "        self._plot_results(results, [\"time_real\", \"time_cpu\", \"memory\"])\n",
    "\n",
    "    # Función general de plots\n",
    "    def _plot_results(self, data, plot_types, x_axis=None, log_base=10):\n",
    "        if x_axis is None:\n",
    "            x_axis = self.sizes\n",
    "        \n",
    "        for plot_type in plot_types:\n",
    "            fig, axs = plt.subplots(1, len(self.classes), figsize=(6*len(self.classes), 6))\n",
    "            if len(self.classes) == 1:\n",
    "                axs = [axs]\n",
    "            \n",
    "            for i, db_name in enumerate(self.classes):\n",
    "                ax = axs[i]\n",
    "                operations = list(data[db_name].keys())\n",
    "                for operation in operations:\n",
    "                    y_values = []\n",
    "                    for el in data[db_name][operation]:\n",
    "                        if plot_type == \"memory\":\n",
    "                            y_values.append(el[2])\n",
    "                        elif plot_type == \"time_cpu\":\n",
    "                            y_values.append(el[1])\n",
    "                        else:\n",
    "                            y_values.append(el[0])\n",
    "                    \n",
    "                    ax.plot(x_axis, y_values, label=operation.capitalize(), marker='o')\n",
    "                \n",
    "                ax.set_title(f'{db_name.capitalize()}', fontsize=16)\n",
    "                ax.set_xlabel('Tamaño del dataset', fontsize=14)\n",
    "                ax.set_xticks(x_axis)\n",
    "                ax.set_xticklabels(x_axis)\n",
    "                ax.set_xscale(\"log\", base=log_base)\n",
    "                ax.grid(True)\n",
    "                if len(operations) > 1:\n",
    "                    ax.legend()\n",
    "\n",
    "            titles = {\n",
    "                \"memory\": \"Uso de Memoria\",\n",
    "                \"time_real\": \"Tiempo Real\",\n",
    "                \"time_cpu\": \"Tiempo de CPU\"\n",
    "            }\n",
    "            labels = {\n",
    "                \"memory\": \"Memoria (MB)\",\n",
    "                \"time_real\": \"Tiempo (segundos)\",\n",
    "                \"time_cpu\": \"Tiempo (segundos)\"\n",
    "            }\n",
    "            \n",
    "            plt.suptitle(titles[plot_type], fontsize=16)\n",
    "            fig.supylabel(labels[plot_type], x=0, fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    \"mongo\": MongoDB,\n",
    "    \"sqlite\": Sqlite3DB,\n",
    "    \"duckdb\": DuckDB,\n",
    "    \"postgres\": PostgresqlDB,\n",
    "}\n",
    "\n",
    "measurements = Measurements(classes, \"Practica_1\", \"users\", [\"dni\"], [10**n for n in range(3, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements.plot_inserts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements.plot_reads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements.plot_updates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
