{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pymongo\n",
                "import json\n",
                "import pandas as pd\n",
                "import sqlite3\n",
                "import psycopg2\n",
                "import duckdb\n",
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "from memory_profiler import memory_usage\n",
                "import numpy as np\n",
                "from sqlalchemy import create_engine, text\n",
                "import json\n",
                "from functools import partial\n",
                "from pymemcache.client import base\n",
                "from pymemcache import serde\n",
                "import re\n",
                "from abc import ABC, abstractmethod\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "cache = base.Client(('127.0.0.1', 11211),\n",
                "    serde=serde.pickle_serde)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clase abstracta con la funcionalidad común de las bases de datos\n",
                "class DB(ABC):\n",
                "    def __init__(self, dbname, table_name, unique_cols, query_type=\"sql\", close_conn_after_op=False):\n",
                "        self.dbname = dbname\n",
                "        self.table_name = table_name\n",
                "        self._unique_cols = unique_cols or []\n",
                "        self.oid = 0\n",
                "        self._cached_queries = set()\n",
                "        self._query_type = query_type\n",
                "        self.close_conn_after_op = close_conn_after_op\n",
                "        if self.close_conn_after_op:\n",
                "            try:\n",
                "                self._open_connection()\n",
                "                self._delete_table()\n",
                "            finally:\n",
                "                self._close_connection()\n",
                "        else:\n",
                "            self._open_connection()\n",
                "            self._delete_table()\n",
                "\n",
                "    @abstractmethod\n",
                "    def _delete_table(self):\n",
                "        pass\n",
                "\n",
                "    def _get_cache_key(self, query):\n",
                "        # La llave de la caché usa el nombre de la implementación concreta para poder hacer pruebas con todas a la vez\n",
                "        return self.__class__.__name__ + \"/\" + re.sub(r'\\s+', '#', str(query))\n",
                "\n",
                "    def _clear_cache(self):\n",
                "        for key in list(self._cached_queries):\n",
                "            cache.delete(key)\n",
                "        self._cached_queries = set()\n",
                "\n",
                "    def insert(self, data_csv, data_json, *_, oid=None, batch_size=None, **__):\n",
                "        if oid is None or oid >= self.oid:\n",
                "            self.oid += 1\n",
                "            self._clear_cache()\n",
                "            if batch_size is None:\n",
                "                batch_size = len(data_json) # Si no se especifica un tamaño de batch, insertar todo a la vez\n",
                "            if self.close_conn_after_op:\n",
                "                try:\n",
                "                    self._open_connection()\n",
                "                    return self._insert(data_csv, data_json, batch_size)\n",
                "                finally:\n",
                "                    self._close_connection()\n",
                "            else:\n",
                "                return self._insert(data_csv, data_json, batch_size)\n",
                "\n",
                "    @abstractmethod\n",
                "    def _insert(self, data_csv, data_json, batch_size):\n",
                "        pass\n",
                "\n",
                "    def read(self, *_, oid=None, query=None, use_cache=True, **__):\n",
                "        if oid is None or oid >= self.oid:\n",
                "            self.oid += 1\n",
                "            # Usar un select all como query predeterminada\n",
                "            query = query or (f'SELECT * FROM {self.table_name};' if self._query_type == \"sql\" else {})\n",
                "            key = self._get_cache_key(query)\n",
                "            cached_data = cache.get(key)\n",
                "            if use_cache and cached_data is not None:\n",
                "                return cached_data\n",
                "            if self.close_conn_after_op:\n",
                "                try:\n",
                "                    self._open_connection()\n",
                "                    data = self._read(query)\n",
                "                finally:\n",
                "                    self._close_connection()\n",
                "            else:\n",
                "                data = self._read(query)\n",
                "            cache.set(key, data)\n",
                "            self._cached_queries.add(key)\n",
                "            return data\n",
                "\n",
                "    @abstractmethod\n",
                "    def _read(self, query):\n",
                "        pass\n",
                "\n",
                "    def update(self, *_, oid=None, **__):\n",
                "        if oid is None or oid >= self.oid:\n",
                "            self.oid += 1\n",
                "            self._clear_cache()\n",
                "            if self.close_conn_after_op:\n",
                "                try:\n",
                "                    self._open_connection()\n",
                "                    return self._update()\n",
                "                finally:\n",
                "                    self._close_connection()\n",
                "            else:\n",
                "                self._update()\n",
                "\n",
                "    @abstractmethod\n",
                "    def _update(self):\n",
                "        pass\n",
                "\n",
                "    def to_df(self):\n",
                "        if self.close_conn_after_op:\n",
                "            try:\n",
                "                self._open_connection()\n",
                "                return self._to_df()\n",
                "            finally:\n",
                "                self._close_connection()\n",
                "        else:\n",
                "            return self._to_df()\n",
                "\n",
                "    @abstractmethod\n",
                "    def _to_df(self):\n",
                "        pass\n",
                "\n",
                "    @abstractmethod\n",
                "    def _open_connection(self):\n",
                "        pass\n",
                "\n",
                "    @abstractmethod\n",
                "    def _close_connection(self):\n",
                "        pass\n",
                "\n",
                "    def create_index(self, index_cols=None):\n",
                "        if index_cols is None:\n",
                "            index_cols = self._unique_cols\n",
                "\n",
                "        if self.close_conn_after_op:\n",
                "            try:\n",
                "                self._open_connection()\n",
                "                return self._create_index(index_cols)\n",
                "            finally:\n",
                "                self._close_connection()\n",
                "        else:\n",
                "            return self._create_index(index_cols)\n",
                "\n",
                "    @abstractmethod\n",
                "    def _create_index(self, index_cols):\n",
                "        pass\n",
                "\n",
                "    def __del__(self):\n",
                "        self._clear_cache()\n",
                "        if not self.close_conn_after_op:\n",
                "            self._close_connection()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clase para calcular tiempos en el gestor de MongoDB\n",
                "class MongoDB(DB):\n",
                "    def __init__(self, dbname, table_name, unique_cols=None, close_conn_after_op=False):\n",
                "        super().__init__(dbname, table_name, unique_cols, query_type=\"mongo\", close_conn_after_op=close_conn_after_op)\n",
                "\n",
                "    def _delete_table(self):\n",
                "        self.db.drop_collection(self.table_name)\n",
                "\n",
                "    def _insert(self, _, data_json, batch_size):\n",
                "        for i in range(0, len(data_json), batch_size):\n",
                "            self.collection.insert_many(data_json[i:i+batch_size]) # Inserta los registros en batches\n",
                "\n",
                "    def _read(self, query):\n",
                "        return list(self.collection.find(query))\n",
                "\n",
                "    def _update(self):\n",
                "        \"\"\"\n",
                "        Actualiza el campo 'dni' añadiendo un 0 al final de cada valor para todos los documentos en la colección.\n",
                "        La actualización se realiza de una sola vez para todos los documentos.\n",
                "        \"\"\"\n",
                "        # Busca todos los documentos y selecciona el campo 'dni'\n",
                "        documents = self.collection.find({}, {\"dni\": 1})\n",
                "\n",
                "        bulk_updates = []\n",
                "\n",
                "        for document in documents:\n",
                "            current_dni = document.get('dni', '')\n",
                "            new_dni = str(current_dni) + '0'\n",
                "\n",
                "            # Preparar la operación de actualización en bloque\n",
                "            bulk_updates.append(\n",
                "                pymongo.UpdateOne(\n",
                "                    {\"_id\": document[\"_id\"]}, # Filtra por el ID del documento\n",
                "                    {\"$set\": {\"dni\": new_dni}} # Establece el nuevo valor de 'dni'\n",
                "                )\n",
                "            )\n",
                "\n",
                "        if bulk_updates:\n",
                "            self.collection.bulk_write(bulk_updates)\n",
                "\n",
                "    def _close_connection(self):\n",
                "        if self._client:\n",
                "            self._client.close()\n",
                "            self._client = None\n",
                "\n",
                "    def _open_connection(self):\n",
                "        self._client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
                "        self.db = self._client[self.dbname]\n",
                "        self.collection = self.db[self.table_name]\n",
                "\n",
                "    def _to_df(self):\n",
                "        return pd.DataFrame(self.read()) # Usa la query predeterminada para obtener la base de datos entera\n",
                "\n",
                "    def _create_index(self, index_cols):\n",
                "        for col in index_cols:\n",
                "            self.collection.create_index([(col, pymongo.ASCENDING)], unique=True)\n",
                "\n",
                "\n",
                "# Clase para calcular tiempos en el gestor de PostgreSQL\n",
                "class PostgresqlDB(DB):\n",
                "    def __init__(self, dbname, table_name, unique_cols=None, close_conn_after_op=False):\n",
                "        super().__init__(dbname, table_name, unique_cols, close_conn_after_op=close_conn_after_op)\n",
                "\n",
                "    def _delete_table(self):\n",
                "        self.cursor.execute(text(f\"DROP TABLE IF EXISTS {self.table_name};\"))\n",
                "        self.cursor.commit()\n",
                "\n",
                "    def _create_table(self, data_csv):\n",
                "        \"\"\"\n",
                "        Create a table based on the DataFrame's columns and types, with unique constraints on specified columns.\n",
                "        \"\"\"\n",
                "        # Mapping de pandas a PostgreSQL\n",
                "        type_mapping = {\n",
                "            'int64': 'INTEGER',\n",
                "            'float64': 'FLOAT',\n",
                "            'object': 'TEXT',\n",
                "            'bool': 'BOOLEAN',\n",
                "            'datetime64[ns]': 'TIMESTAMP',\n",
                "            'timedelta[ns]': 'INTERVAL'\n",
                "        }\n",
                "\n",
                "        columns_with_types = []\n",
                "        for col, dtype in data_csv.dtypes.items():\n",
                "            sql_type = type_mapping.get(str(dtype), 'TEXT') # Usar TEXT por defecto\n",
                "            columns_with_types.append(f\"{col} {sql_type}\")\n",
                "\n",
                "        # if self._unique_cols:\n",
                "        #     unique_str = \", \".join([f\"UNIQUE({col})\" for col in self._unique_cols])\n",
                "        #     columns_with_types.append(unique_str)\n",
                "\n",
                "        # Generar el texto de creación de la tabla\n",
                "        create_table_query = f\"\"\"\n",
                "        CREATE TABLE {self.table_name} (\n",
                "            {', '.join(columns_with_types)}\n",
                "        );\n",
                "        \"\"\"\n",
                "\n",
                "        self.cursor.execute(text(create_table_query))\n",
                "        self.cursor.commit()\n",
                "\n",
                "    def _insert(self, data_csv, _, batch_size):\n",
                "        \"\"\"\n",
                "        Inserta los datos de un DataFrame de pandas (ya cargado con read_csv) en la tabla de PostgreSQL.\n",
                "        \"\"\"\n",
                "        self._create_table(data_csv)\n",
                "        \n",
                "        # Query para insertar valores\n",
                "        columns = ', '.join(data_csv.columns)\n",
                "        placeholders = ', '.join([f':{col}' for col in data_csv.columns])\n",
                "        insert_query = f\"\"\"\n",
                "        INSERT INTO {self.table_name} ({columns})\n",
                "        VALUES ({placeholders})\n",
                "        \"\"\"\n",
                "        \n",
                "        data = data_csv.to_dict('records')\n",
                "        \n",
                "        for i in range(0, len(data), batch_size): # Insertar por batches\n",
                "            self.cursor.execute(text(insert_query), data[i:i+batch_size])\n",
                "            self.cursor.commit()\n",
                "\n",
                "    def _read(self, query):\n",
                "        return pd.read_sql_query(query, self.conn)\n",
                "\n",
                "    def _update(self):\n",
                "        \"\"\"\n",
                "        Actualiza el campo 'dni' añadiendo un 0 al final de cada valor para todos los registros en la tabla.\n",
                "        \"\"\"\n",
                "        update_query = f\"\"\"\n",
                "        UPDATE {self.table_name}\n",
                "        SET dni = dni || '0';\n",
                "        \"\"\"\n",
                "        self.cursor.execute(text(update_query))\n",
                "        self.cursor.commit()\n",
                "\n",
                "    def _close_connection(self):\n",
                "        if self.cursor:\n",
                "            self.cursor.close()\n",
                "            self.cursor = None\n",
                "\n",
                "    def _open_connection(self):\n",
                "        self.conn = create_engine(f'postgresql+psycopg2://postgres:postgres@localhost:5432/{self.dbname}')\n",
                "        self.cursor = self.conn.connect()\n",
                "\n",
                "    def _to_df(self):\n",
                "        return pd.read_sql_query(f'SELECT * FROM {self.table_name};', self.conn)\n",
                "\n",
                "    def _create_index(self, index_cols=None):\n",
                "        for col in index_cols:\n",
                "            index_query = f\"CREATE INDEX idx_{self.table_name}_{col} ON {self.table_name} ({col});\"\n",
                "            self.cursor.execute(text(index_query))\n",
                "        self.cursor.commit()\n",
                "\n",
                "\n",
                "# Clase para calcular tiempos en el gestor de Sqlite3DB\n",
                "class Sqlite3DB(DB):\n",
                "    def __init__(self, dbname, table_name, unique_cols=None, close_conn_after_op=False):\n",
                "        super().__init__(dbname + \".sqlite3\", table_name, unique_cols, close_conn_after_op=close_conn_after_op)\n",
                "\n",
                "    def _delete_table(self):\n",
                "        self.conn.execute(f\"DROP TABLE IF EXISTS {self.table_name};\") # Limpiar la tabla antes de insertar\n",
                "        self.conn.commit()  # Aplicar los cambios\n",
                "\n",
                "    def _create_table(self, data_csv):\n",
                "        \"\"\"\n",
                "        Create a table based on the DataFrame's columns and types, with unique constraints on specified columns.\n",
                "        \"\"\"\n",
                "        # Mapping de pandas a SQLite\n",
                "        type_mapping = {\n",
                "            'int64': 'INTEGER',\n",
                "            'float64': 'REAL',\n",
                "            'object': 'TEXT',\n",
                "            'bool': 'BOOLEAN',\n",
                "            'datetime64[ns]': 'TEXT'\n",
                "        }\n",
                "\n",
                "        columns_with_types = []\n",
                "        for col, dtype in data_csv.dtypes.items():\n",
                "            sql_type = type_mapping.get(str(dtype), 'TEXT') # Usar TEXT por defecto\n",
                "            columns_with_types.append(f\"{col} {sql_type}\")\n",
                "\n",
                "        # if self._unique_cols:\n",
                "        #     unique_str = \", \".join([f\"UNIQUE({col})\" for col in self._unique_cols])\n",
                "        #     columns_with_types.append(unique_str)\n",
                "\n",
                "        # Generar el texto de creación de la tabla\n",
                "        create_table_query = f\"\"\"\n",
                "        CREATE TABLE {self.table_name} (\n",
                "            {', '.join(columns_with_types)}\n",
                "        );\n",
                "        \"\"\"\n",
                "\n",
                "        self.conn.execute(create_table_query)\n",
                "        self.conn.commit()\n",
                "\n",
                "    def _insert(self, data_csv, _, batch_size):\n",
                "        self._create_table(data_csv)\n",
                "        \n",
                "        columns = ', '.join(data_csv.columns)\n",
                "        placeholders = ', '.join(['?' for _ in data_csv.columns])\n",
                "        insert_query = f\"\"\"\n",
                "        INSERT INTO {self.table_name} ({columns})\n",
                "        VALUES ({placeholders})\n",
                "        \"\"\"\n",
                "        \n",
                "        data = [tuple(x) for x in data_csv.to_numpy()]\n",
                "        \n",
                "        for i in range(0, len(data), batch_size): # Insertar por batches\n",
                "            self.cursor.executemany(insert_query, data[i:i+batch_size])\n",
                "            self.conn.commit()\n",
                "\n",
                "    def _read(self, query):\n",
                "        self.cursor.execute(query)\n",
                "        return self.cursor.fetchall()\n",
                "\n",
                "    def _update(self):\n",
                "        \"\"\"\n",
                "        Actualiza el campo 'dni' añadiendo un 0 al final de cada valor para todos los registros en la tabla.\n",
                "        \"\"\"\n",
                "        update_query = f\"\"\"\n",
                "        UPDATE {self.table_name}\n",
                "        SET dni = dni || '0';\n",
                "        \"\"\"\n",
                "        self.cursor.execute(update_query)\n",
                "        self.conn.commit()\n",
                "\n",
                "    def _close_connection(self):\n",
                "        if self.conn:\n",
                "            self.conn.close()\n",
                "            self.conn = None\n",
                "\n",
                "    def _open_connection(self):\n",
                "        self.conn = sqlite3.connect(self.dbname)\n",
                "        self.cursor = self.conn.cursor()\n",
                "\n",
                "    def _to_df(self):\n",
                "        # Convertir los resultados en un DataFrame\n",
                "        return pd.DataFrame(self.read(), columns=[desc[0] for desc in self.cursor.description])\n",
                "\n",
                "    def _create_index(self, index_cols=None):\n",
                "        for col in index_cols:\n",
                "            index_query = f\"CREATE INDEX idx_{self.table_name}_{col} ON {self.table_name} ({col});\"\n",
                "            self.conn.execute(index_query)\n",
                "        self.conn.commit()\n",
                "\n",
                "\n",
                "# Clase para calcular tiempos en el gestor de DuckDB\n",
                "class DuckDB(DB):\n",
                "    def __init__(self, dbname, table_name, unique_cols=None, close_conn_after_op=False):\n",
                "        super().__init__(dbname, table_name, unique_cols, close_conn_after_op=close_conn_after_op)\n",
                "\n",
                "    def _delete_table(self):\n",
                "        self.conn.execute(f\"DROP TABLE IF EXISTS {self.table_name};\")\n",
                "\n",
                "    def _create_table(self, data_csv, index_cols=None):\n",
                "        \"\"\"\n",
                "        Create a table based on the DataFrame's columns and types, with unique constraints on specified columns.\n",
                "        \"\"\"\n",
                "        # Mapping de pandas a DuckDB\n",
                "        type_mapping = {\n",
                "            'int64': 'INTEGER',\n",
                "            'float64': 'DOUBLE',\n",
                "            'object': 'TEXT',\n",
                "            'bool': 'BOOLEAN',\n",
                "            'datetime64[ns]': 'TIMESTAMP'\n",
                "        }\n",
                "\n",
                "        columns_with_types = []\n",
                "        for col, dtype in data_csv.dtypes.items():\n",
                "            sql_type = type_mapping.get(str(dtype), 'TEXT') # Usar TEXT por defecto\n",
                "            columns_with_types.append(f\"{col} {sql_type}\")\n",
                "\n",
                "        if index_cols:\n",
                "            unique_str = \", \".join([f\"UNIQUE({col})\" for col in index_cols])\n",
                "            columns_with_types.append(unique_str)\n",
                "\n",
                "        # Generar el texto de creación de la tabla\n",
                "        create_table_query = f\"\"\"\n",
                "        CREATE TABLE {self.table_name} (\n",
                "            {', '.join(columns_with_types)}\n",
                "        );\n",
                "        \"\"\"\n",
                "\n",
                "        self.conn.execute(create_table_query)\n",
                "\n",
                "    def _insert(self, data_csv, _, batch_size, index_cols=None):\n",
                "        self._create_table(data_csv, index_cols)\n",
                "        self.data = data_csv\n",
                "        \n",
                "        columns = ', '.join(data_csv.columns)\n",
                "        placeholders = ', '.join(['?' for _ in data_csv.columns])\n",
                "        insert_query = f\"\"\"\n",
                "        INSERT INTO {self.table_name} ({columns})\n",
                "        VALUES ({placeholders})\n",
                "        \"\"\"\n",
                "        \n",
                "        data = [tuple(x) for x in data_csv.to_numpy()]\n",
                "        \n",
                "        for i in range(0, len(data), batch_size): # Insertar por batches\n",
                "            self.conn.executemany(insert_query, data[i:i+batch_size])\n",
                "            self.conn.commit()\n",
                "\n",
                "    def _read(self, query):\n",
                "        self.cursor.execute(query)\n",
                "        return self.cursor.fetchall()\n",
                "\n",
                "    def _update(self):\n",
                "        \"\"\"\n",
                "        Actualiza el campo 'dni' añadiendo un 0 al final de cada valor para todos los registros en la tabla.\n",
                "        \"\"\"\n",
                "        update_query = f\"\"\"\n",
                "        UPDATE {self.table_name}\n",
                "        SET dni = dni || '0';\n",
                "        \"\"\"\n",
                "        self.cursor.execute(update_query)\n",
                "        self.conn.commit()\n",
                "\n",
                "    def _close_connection(self):\n",
                "        if self.conn:\n",
                "            self.conn.close()\n",
                "            self.conn = None\n",
                "\n",
                "    def _open_connection(self):\n",
                "        self.conn = duckdb.connect(self.dbname)\n",
                "        self.cursor = self.conn.cursor()\n",
                "\n",
                "    def _to_df(self):\n",
                "        return pd.DataFrame(self.read(), columns=[desc[0] for desc in self.cursor.description])\n",
                "\n",
                "    def _create_index(self, index_cols=None):\n",
                "        self._delete_table()\n",
                "        self._insert(self.data, None, self.data.shape[0], index_cols)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Measurements:\n",
                "    def __init__(self, classes, dbname, table_name, unique_cols, sizes):\n",
                "        self.classes = classes\n",
                "        self.sizes = sizes\n",
                "        self.dbname = dbname\n",
                "        self.table_name = table_name\n",
                "        self._unique_cols = unique_cols\n",
                "\n",
                "    def _read_csv_files(self, n):\n",
                "        return pd.read_csv(f'results/{self.table_name}/csv/{self.table_name}_{n}.csv')\n",
                "\n",
                "    def _read_json_files(self, n):\n",
                "        with open(f'results/{self.table_name}/json/{self.table_name}_{n}.json', 'r', encoding='utf-8') as f:\n",
                "            return json.load(f)\n",
                "\n",
                "    def _measure_time_and_memory(self, function):\n",
                "        # Se mide la memoria de la función de medición de tiempos\n",
                "        # De hacerlo al revés, todas las mediciones tardarían unos 6 segundos más por el tiempo de inicialización de `memory_usage`\n",
                "        \n",
                "        # Se usa una lista para poder sacar los resultados de la función interna\n",
                "        result = []\n",
                "        def _measure_time(function):\n",
                "            t1 = time.perf_counter(), time.process_time()\n",
                "            function()\n",
                "            t2 = time.perf_counter(), time.process_time()\n",
                "            result.append((t2[0] - t1[0], t2[1] - t1[1]))\n",
                "        memory = memory_usage(lambda: _measure_time(function))\n",
                "        return result[0][0], result[0][1], np.average(memory)\n",
                "\n",
                "    def _get_time_memory(self, n, close_conn_after_op):\n",
                "        data_json = self._read_json_files(n)\n",
                "        data_csv = self._read_csv_files(n)\n",
                "        results = {cl: {\"insert\": [], \"read\": [], \"update\": []} for cl in self.classes}\n",
                "\n",
                "        for cl, cl_class in self.classes.items():\n",
                "            cl_obj = cl_class(self.dbname, self.table_name, self._unique_cols, close_conn_after_op=close_conn_after_op)\n",
                "            print(f\"\\n{cl}{' ' * (20 - len(cl) - len(str(n)))}{n}\", end=\"\")\n",
                "\n",
                "            # Tiempo y memoria de cada operación por tamaño del dataset\n",
                "            for operation in ['insert', 'read', 'update']:\n",
                "                print(f\" | {operation} \", end=\"\")\n",
                "                oid = cl_obj.oid\n",
                "                t_real, t_cpu, memory = self._measure_time_and_memory(\n",
                "                    partial(getattr(cl_obj, operation), data_csv, data_json, oid=oid, use_cache=False)\n",
                "                )\n",
                "                results[cl][operation].append((t_real, t_cpu, memory))\n",
                "                t_str = f\"{t_real:.5f}\"\n",
                "                print(f\"{t_str:>10}s\", end=\"\")\n",
                "\n",
                "        return results\n",
                "\n",
                "    def _get_cache_times(self, n):\n",
                "        data_json = self._read_json_files(n)\n",
                "        data_csv = self._read_csv_files(n)\n",
                "        results = {cl: {\"no cache\": [], \"index\": [], \"cache\": []} for cl in self.classes}\n",
                "\n",
                "        # Tiempo de lectura con diferentes configuraciones para cada tamaño de dataset\n",
                "        for cl, cl_class in self.classes.items():\n",
                "            cl_obj = cl_class(self.dbname, self.table_name, self._unique_cols)\n",
                "            print(f\"\\n{cl}{' ' * (20 - len(cl) - len(str(n)))}{n}\", end=\"\")\n",
                "            cl_obj.insert(data_csv, data_json, oid=cl_obj.oid)\n",
                "\n",
                "            column = cl_obj._unique_cols[0]\n",
                "            for cache_type in [\"no cache\", \"index\", \"cache\"]:\n",
                "                if cache_type == \"index\":\n",
                "                    cl_obj.create_index()\n",
                "                \n",
                "                print(f\" | {cache_type} \", end=\"\")\n",
                "                t = time.time()\n",
                "                times = []\n",
                "                for value in data_csv[column][-200:]:\n",
                "                    query = f\"SELECT * FROM table WHERE {column} = '{value}'\" if cl_obj._query_type == \"sql\" else {column: value}\n",
                "                    t_real, t_cpu = self._measure_time(\n",
                "                        partial(cl_obj.read, query, oid=cl_obj.oid, use_cache=(cache_type == \"cache\"))\n",
                "                    )\n",
                "                    times.append((t_real, t_cpu))\n",
                "                t_str = f\"{time.time() - t:.5f}\"\n",
                "                print(f\"{t_str:>10}s\", end=\"\")\n",
                "                results[cl][cache_type].append((np.mean([t[0] for t in times]), np.mean([t[1] for t in times])))\n",
                "\n",
                "        return results\n",
                "\n",
                "    def _measure_time(self, function):\n",
                "        t1 = time.perf_counter(), time.process_time()\n",
                "        function()\n",
                "        t2 = time.perf_counter(), time.process_time()\n",
                "        return t2[0] - t1[0], t2[1] - t1[1]\n",
                "\n",
                "    def plot_time_memory(self, by_database=False, show_cpu=False):\n",
                "        time_memory_data = {size: self._get_time_memory(size, False) for size in self.sizes}\n",
                "        self._plot_results(time_memory_data, plot_types=[\"time_real\", \"time_cpu\", \"memory\"] if show_cpu else [\"time_real\", \"memory\"], by_database=by_database)\n",
                "\n",
                "    def plot_cache_times(self, by_database=True, show_cpu=False):\n",
                "        cache_time_data = {size: self._get_cache_times(size) for size in self.sizes}\n",
                "        self._plot_results(cache_time_data, plot_types=[\"time_real\", \"time_cpu\"] if show_cpu else [\"time_real\"], by_database=by_database)\n",
                "\n",
                "    # Función general de plots\n",
                "    def _plot_results(self, data, plot_types, by_database=False):\n",
                "        operations = list(data[self.sizes[0]][list(self.classes.keys())[0]].keys())\n",
                "        \n",
                "        for plot_type in plot_types:\n",
                "            if by_database:\n",
                "                fig, axs = plt.subplots(1, len(self.classes), figsize=(6*len(self.classes), 6))\n",
                "                if len(self.classes) == 1:\n",
                "                    axs = [axs]\n",
                "                \n",
                "                for i, db_name in enumerate(self.classes):\n",
                "                    ax = axs[i]\n",
                "                    for operation in operations:\n",
                "                        y_values = []\n",
                "                        for size in self.sizes:\n",
                "                            if plot_type == \"memory\":\n",
                "                                y_values.append(data[size][db_name][operation][0][2])  # memory\n",
                "                            elif plot_type == \"time_cpu\":\n",
                "                                y_values.append(data[size][db_name][operation][0][1])  # CPU time\n",
                "                            else:  # time_real\n",
                "                                y_values.append(data[size][db_name][operation][0][0])  # real time\n",
                "                        \n",
                "                        ax.plot(self.sizes, y_values, label=operation.capitalize(), marker='o')\n",
                "                    \n",
                "                    ax.set_title(f'{db_name.capitalize()}', fontsize=16)\n",
                "                    ax.set_xlabel('Tamaño del dataset', fontsize=14)\n",
                "                    ax.set_xticks(self.sizes)\n",
                "                    ax.set_xticklabels(self.sizes)\n",
                "                    ax.set_xscale(\"log\")\n",
                "                    ax.grid(True)\n",
                "                    ax.legend()\n",
                "            else:\n",
                "                fig, axs = plt.subplots(1, len(operations), figsize=(6*len(operations), 6))\n",
                "                if len(operations) == 1:\n",
                "                    axs = [axs]\n",
                "                \n",
                "                for i, operation in enumerate(operations):\n",
                "                    ax = axs[i]\n",
                "                    for db_name in self.classes:\n",
                "                        y_values = []\n",
                "                        for size in self.sizes:\n",
                "                            if plot_type == \"memory\":\n",
                "                                y_values.append(data[size][db_name][operation][0][2])  # memory\n",
                "                            elif plot_type == \"time_cpu\":\n",
                "                                y_values.append(data[size][db_name][operation][0][1])  # CPU time\n",
                "                            else:  # time_real\n",
                "                                y_values.append(data[size][db_name][operation][0][0])  # real time\n",
                "                        \n",
                "                        ax.plot(self.sizes, y_values, label=db_name.capitalize(), marker='o')\n",
                "                    \n",
                "                    ax.set_title(f'{operation.capitalize()}', fontsize=16)\n",
                "                    ax.set_xlabel('Tamaño del Dataset', fontsize=14)\n",
                "                    ax.set_xticks(self.sizes)\n",
                "                    ax.set_xticklabels(self.sizes)\n",
                "                    ax.set_xscale(\"log\")\n",
                "                    ax.grid(True)\n",
                "                    ax.legend()\n",
                "\n",
                "            titles = {\n",
                "                \"memory\": \"Uso de Memoria\",\n",
                "                \"time_real\": \"Tiempo Real\",\n",
                "                \"time_cpu\": \"Tiempo de CPU\"\n",
                "            }\n",
                "            labels = {\n",
                "                \"memory\": \"Memoria (MB)\",\n",
                "                \"time_real\": \"Tiempo (segundos)\",\n",
                "                \"time_cpu\": \"Tiempo (segundos)\"\n",
                "            }\n",
                "            \n",
                "            plt.suptitle(titles[plot_type], fontsize=18)\n",
                "            fig.supylabel(labels[plot_type], x=0, fontsize=18)\n",
                "            plt.tight_layout()\n",
                "            plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "classes = {\n",
                "    \"mongo\": MongoDB,\n",
                "    \"sqlite\": Sqlite3DB,\n",
                "    \"duckdb\": DuckDB,\n",
                "    \"postgres\": PostgresqlDB,\n",
                "}\n",
                "\n",
                "measurements = Measurements(classes, \"Practica_1\", \"cars\", [\"vin\", \"plate\"], [10**n for n in range(3, 6)]) # 3, 7"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "measurements.plot_time_memory(show_cpu=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "measurements.plot_cache_times()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
